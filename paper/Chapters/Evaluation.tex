\chapter{Evaluation}

\label{Chapter6}

In order to evaluate the performance of the forked version of Moonlight developed for this paper, it must be tested against the other existing solutions.
The testing methodology employed to ensure the accuracy of the testing is described in Section \ref{sec:TestingMethodology}.
Then, the responsivity and latency of each solution is evaluated in Section \ref{sec:ResponsivityLatency}, and the quality of each solution's stream is evaluated in Section \ref{sec:Quality}.
Then, the forked version of Moonlight is evaluated in real world scenarios in Section \ref{sec:RealWorldTesting}.
Finally, the results are summarized and verdicts on each of the solutions' applicability to the research questions will be presented in Section \ref{sec:EvaluationSummary}.


\section{Testing Methodology}\label{sec:TestingMethodology}

In order to ensure that the data collected for analysis is accurate and directly comparable, the testing environment must be kept exactly the same between trials and between different applications.
This gives each application the exact same starting conditions, and the best chance to demonstrate it's unabated performance.
For these tests, the host machine is a desktop computer running Windows 10 with a AMD Ryzen 9 5900X CPU and a Nvidia RTX 3070 GPU running the latest GeForce Experience version \emph{3.25.0.84}.
This configuration does not change throughout testing.
The client machine is the CM4 board attached to the PCB developed in Chapter \ref{Chapter4}, running the custom Manjaro linux distribution and forked version of Moonlight developed in Chapter \ref{Chapter5}.
This configuration also does not change throughout testing.
Each test is performed with both the host computer and the client machine running on the same network, with both devices connected to the same router using an ethernet cable.
This ensures that the speed of the network is not a factor in the tests.
The tests are performed multiple times under the following conditions:

\begin{itemize}
  \item Ideal Conditions: No other applications are running during testing.
  \item CPU Stressed: The CPU is constantly at 100\% load during the entire test.
  \item GPU Stressed: The GPU is constantly at 100\% load during the entire test.
  \item Both Stressed: Both the CPU and the GPU are constantly at 100\% load during the entire test.
\end{itemize}

\noindent
This is to examine whether each application can perform under intensive conditions.
For each test, the following steps are performed:

\begin{enumerate}
  \item Both the host and client computers are fully rebooted to ensure no other application is running.
  \item The host computer starts up a single browser window with the web page described in Section \ref{sec:DevelopingTestingAndMeasurementTools}.
  \item If the test calls for the CPU or the GPU to be stressed, the application \enquote{Blender} is started on the host machine, and a benchmark is performed for the CPU or the GPU.
        This benchmark will run for longer than the duration of the test, and will keep the CPU and/or GPU at 100\% load for the entire duration of the benchmark.
  \item Both devices start up their respective data collection tool that will record keystrokes and changes in the screen's color.
  \item The client machine connects to the host computer using the application being tested.
  \item The tester presses the \enquote{=} key on the client machine to begin the automated testing process.
        Due to the wide variance in what the user can do using the software, it is nearly impossible to test every possible real world scenario.
        Instead, the automated test simulates an active working environment by sending a repeatable series of keystrokes that cause the entire screen to change colors to mimic high-intensity applications.
        For consistent testing, 400 keystrokes are automatically sent with a 500ms delay between inputs.
  \item Once the trial is complete, both the client and the host computers close their data recording tools and the client ends the streaming session.
\end{enumerate}

\noindent
This process is repeated for each of the following applications:

\begin{itemize}
  \item The forked version of Moonlight developed for this project.
  \item Chrome Remote Desktop (CRD, Section \ref{subsec:ChromeRemoteDesktop}).
  \item Microsoft's Remote Desktop Connection (RDP, Section \ref{subsec:RemoteDesktopProtocol}).
  \item Virtual Network Computing (VNC, Section \ref{subsec:VirtualNetworkComputing}).
\end{itemize}

\noindent
Once all the data is collected, the data from the host and client machines for a single trial are organized into a single CSV file for processing.
In order to ensure that the data is comparable, all extraneous factors must be removed from the data.
Namely, the network latency and difference in computer clocks must be removed.
By keeping the host and client computers attached to the same network through ethernet, the network latency is minimized.
But the task of keeping two computer's internal clocks synchronized is not trivial.
Even after instructing the operating system to synchronize its clock with an internet time server, the two computers may still report different times.
In some cases, the difference between the host and client computer's system time was noticed to be upwards of a full second after resynchronizing both clocks \cite{time.is}.
This is partially accounted for in step \emph{6} of the above process, and the remaining inaccuracy can now be removed by subtracting the time difference between the when the client presses the button and the time the host executes that command.
It is important to not subtract the entire round trip time of the input, as that is what is being testing in the following section, but rather just the time between when the client sends the input and when the host receives it.
This ensures that any inaccuracies in each computer's internal clock will not impact the data.

After the data is cleaned up, the data can be processed by matching each keystroke and change in the screen's color detected by the client and host machines and calculating how much time elapsed between the events.
This reveals the amount of delay the user experiences between when they press a key and when the host executes the command, and between when they press a key and can visually see the response.


\section{Responsivity and Latency}\label{sec:ResponsivityAndLatency}

The first factor in determining the performance of the application is to take a look at how responsive it is to use.
Responsivity is defined as how quick the application responds to inputs and how quickly the user can see the results of their inputs.
This is important for general user experience and applications where continuous input is required.
This is also what is generally referred to when determining whether an application has a positive user experience, since a program that can take seconds to respond to the user begets lower productivity and a more frustrating experience.
To analyze this, two points of data are recorded: First, the amount of time between when the client records a keystroke and when it is actually inputted on the host machine, and secondly, the amount of time between the host machine updating their screen and when the client actually displays that update.
The first datum is generally referred to as the latency of the application, since even though each trial is done using the same network setup different solutions may have different optimizations built in that reduce the time needed for the host to implement keystrokes made by the client.
This means that as the network delay is removed from the data, all that is left is the time that it takes the software on the host's machine to actually execute the input.
Each of the following figures shows data over one trial for each category.
Each trial consists of the client machine programmatically sending 400 keystrokes with 500ms between each input.
Limitations in this data is discussed in Section \ref{sec:ConclusionLimitations}.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        boxplot/draw direction=y,
        ylabel={Time (ms)},
        scale only axis,
        height=8cm,
        width=\textwidth,
        ymin=0,ymax=200,
        cycle list={{green!70!black},{red},{blue},{black}},
        legend style={
            legend pos=outer north east,
            font=\small
          },
        legend cell align=left,
        legend image code/.code={%
            \draw[#1] (0cm,-0.1cm) rectangle (0.6cm,0.1cm);
          },
        boxplot={
            % Three boxplots for each column
            draw position={1/5 + floor(\plotnumofactualtype/4) + 1/5*fpumod(\plotnumofactualtype,4)},
            % Each plot takes up a 15% of the column
            box extend=0.15,
          },
        % 1 unit in x controls the width:
        x=2cm,
        % ... and it means that we should describe intervals:
        xtick={0,1,2,...,10},
        x tick label as interval,
        xticklabels={%
            {Forked Moonlight},%
            {CRD},%
            {RDP},%
            {VNC},%
          },
        x tick label style={
            text width=2.5cm,
            align=center
          },
      ]

      % Forked Moonlight
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/moonlight_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/moonlight_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/moonlight_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/moonlight_key_delay.csv};

      % CRD
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/crd_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/crd_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/crd_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/crd_key_delay.csv};

      % RDP
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/rdp_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/rdp_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/rdp_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/rdp_key_delay.csv};

      % VNC
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/vnc_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/vnc_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/vnc_key_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/vnc_key_delay.csv};

      \legend{Ideal,CPU stressed,GPU stressed, Both stressed}
    \end{axis}
  \end{tikzpicture}
  \caption[Input Delay Data]{Time for host to press key from client. n=400.}
  \label{fig:InputDelay}
\end{figure}
Figure \ref{fig:InputDelay} shows the amount of time between the client and the host registering the keystroke under the different test conditions.

Each application does a good job sending data from the client to the host in a timely manner, with the majority of keystrokes being realized within a tenth of a second of being pressed on the client machine.
The forked Moonlight implementation keeps a latency under 20ms regardless of the load of the host machine, with more than 75\% of all keystrokes being realized within 10ms.
CRD proves to be a CPU limited application, with latency while idle or under GPU load also staying below 20ms, but climbing up above 50ms when under CPU load.
This is important to keep in mind when thinking about what sort of situations the host machine will be in while being accessed.
If the user is going to be rendering video remotely, a GPU intensive process, the latency of the stream wont be impacted, but if the user is going to be running a CPU intensive process, the latency can be expected to be negatively impacted.
RDP has a similar results, with extremely low latency while idle or under GPU load, and upwards of 40ms while under CPU load.
VNC has the highest variance, with no discernable affinity to any kind of load.
This lines up with it's intended use described in Section \ref{subsec:VirtualNetworkComputing}, being the simplest and most straightforward method of remote desktop control without much focus on performance or security.
That's not so say VNC is unusable or even difficult to use; latency of 140ms is still usable in all but the most precise situations.

Though this is only one metric to consider, the other is the amount of time it take for the client to display the results of their input.
This is recorded as the amount of time it takes between the host machine changing the color of it's screen and the client rendering the change.
As noted before, the color of the host's screen changes with each input sent by the client, which occurs every 500ms for 400 keystrokes.
Figure \ref{fig:ColorDelay} shows the amount of time for each application under each condition.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        boxplot/draw direction=y,
        ylabel={Time (ms)},
        scale only axis,
        height=8cm,
        width=\textwidth,
        ymin=0,ymax=250,
        cycle list={{green!70!black},{red},{blue},{black}},
        legend style={
            legend pos=outer north east,
            font=\small
          },
        legend cell align=left,
        legend image code/.code={%
            \draw[#1] (0cm,-0.1cm) rectangle (0.6cm,0.1cm);
          },
        boxplot={
            % Three boxplots for each column
            draw position={1/5 + floor(\plotnumofactualtype/4) + 1/5*fpumod(\plotnumofactualtype,4)},
            % Each plot takes up a 15% of the column
            box extend=0.15,
          },
        % 1 unit in x controls the width:
        x=2cm,
        % ... and it means that we should describe intervals:
        xtick={0,1,2,...,10},
        x tick label as interval,
        xticklabels={%
            {Forked Moonlight},%
            {CRD},%
            {RDP},%
            {VNC},%
          },
        x tick label style={
            text width=2.5cm,
            align=center
          },
      ]

      % Forked Moonlight
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/moonlight_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/moonlight_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/moonlight_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/moonlight_color_delay.csv};

      % CRD
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/crd_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/crd_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/crd_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/crd_color_delay.csv};

      % RDP
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/rdp_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/rdp_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/rdp_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/rdp_color_delay.csv};

      % VNC
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/ideal/vnc_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stresscpu/vnc_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressgpu/vnc_color_delay.csv};
      \addplot
      table[x expr=\coordindex+1, y index=0] {Data/stressboth/vnc_color_delay.csv};

      \legend{Ideal,CPU stressed,GPU stressed,Both stressed}
    \end{axis}
  \end{tikzpicture}
  \caption[Color Delay Data]{Time for client to render update to the screen. Values over 250ms push the usability of the system. n=400.}
  \label{fig:ColorDelay}
\end{figure}

This data demonstrates a similar but more extreme pattern to the input delay figure, with most application performing well under ideal conditions but struggling a little under high stress.
The forked Moonlight implementation is able to render updates on the client device in under 70ms in nearly all cases, with an even spread regardless of the load of the host machine.
CRD is able to render all updates in under 160ms while the host is idle, but struggles to keep up while under load.
Again, CRD struggles the most while under CPU load, with the quickest update happening in just under 180ms, and the median update taking over a full second.
CRD performs a little better under GPU load, but still experiences much high delays with the median update taking over half of a second.
Once both the CPU and the GPU are under load, the worst of both scenarios is realized as the median update taking nearly three quarters of a second.
All three of these values push the usability of the system as any kind of intensive program will bring the responsivity of the application to a crawl.
RDP again performs extremely well, with all render updates taking less than 50ms while under ideal conditions or GPU load, and all updates taking less that 70ms while under full CPU load.
VNC is again the slowest and most inconsistent application, with all conditions resulting in a frequent render delays of more than a quarter of a second.
This makes VNC considerably less suitable for the task at hand.

The delay, however, is only half the story in this case.
Another factor to consider is the percentage of updates that are not rendered by the client.
This, often referred to as dropped frames, records how much data is lost when the host sends it's screen to the client device.
A higher percentage results in a more choppy and difficult to use experience, with a 50\% loss meaning that 50\% of the updates sent by the host are not rendered by the client.
Figure \ref{fig:ColorDelayLoss} shows the percentage of color changes not rendered by the client for each of the load conditions.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ylabel={Color changes not rendered (\%)},
        scale only axis,
        height=8cm,
        width=\textwidth,
        enlarge x limits=0.25,
        ybar=2*\pgflinewidth,
        bar width=14pt,
        ymin=0,ymax=100,
        cycle list={{green!70!black},{red},{blue},{black}},
        legend style={
            legend pos=outer north east,
            font=\small
          },
        legend cell align=left,
        legend image code/.code={%
            \draw[#1] (0cm,-0.1cm) rectangle (0.6cm,0.1cm);
          },
        % 1 unit in x controls the width:
        x=2cm,
        % ... and it means that we should describe intervals:
        xtick={0,1,2,...,10},
        % x tick label as interval,
        xticklabels={%
            {Forked Moonlight},%
            {CRD},%
            {RDP},%
            {VNC},%
          },
        x tick label style={
            text width=2.5cm,
            align=center
          },
      ]

      \addplot[style={green!70!black,fill=green!70!black,mark=none}]
      coordinates {(0, 0) (1,0) (2,0) (3,15.32)};

      \addplot[style={red,fill=red,mark=none}]
      coordinates {(0,0) (1,32.28) (2,0) (3,19.51)};

      \addplot[style={blue,fill=blue,mark=none}]
      coordinates {(0,0) (1,2.34) (2,0) (3,15.5)};

      \addplot[style={black,fill=black,mark=none}]
      coordinates {(0, 0) (1,34.65) (2,0) (3,39.83)};

      \legend{Ideal,CPU stressed,GPU stressed, Both stressed}
    \end{axis}
  \end{tikzpicture}
  \caption[Color Delay Loss Data]{Percentage of color changes not rendered. n=400.}
  \label{fig:ColorDelayLoss}
\end{figure}

These data points adds context to the previous figures, showing where the applications struggle to keep up with sending data back to the client device.
The forked Moonlight implementation does not experience any frame drops, as does CRD under ideal conditions.
But once the host machine is put under heavy load, CRD begins to drop frames, with a few being dropped under GPU stress, and 32\% of frames being dropped while under CPU stress.
This can make CRD unpleasant and frustrating to use while the host is running a CPU intensive program as not only does the client take much more time to update the screen to reflect that status of the host computer, but many of the updates are lost completely and not shown to the user.
RDP is also able to display all the data sent by the host without dropping any frames under any condition.
VNC is again proved to be the least performant, with frames being dropped regardless of the load being put on the host computer.
This is still in line with it's intended use of basic remote control, though it is disappointing to see about 15\% loss even under ideal conditions.


\section{Quality}\label{sec:Quality}

Apart from the performance of each application, the quality of the user experience must also be taken into account when considering each application.
Each application will be assessed individually, and then compared in Section \ref{sec:EvaluationSummary}.
Due to the closed nature of some of the protocols, only limited amounts of data could be extracted and compared.
This prevents direct comparisons, but can still contribute to the understanding of the overall performance of the solution.

\subsection{Moonlight}\label{subsec:QualityMoonlight}

When analyzing the forked version of Moonlight, specific quantitative data described in Section \ref{sec:DevelopingTestingAndMeasurementTools} can be collected directly from the application while it is running.
Once the streaming session concludes, the statistics are collated into human-readable numbers and presented in the console.
What follows is an example output from one of the test streaming sessions:

\begin{lstlisting}[style=plaintext,title=Statistics generated after a streaming session]
Global Video Stats
----------------------------------------------------------
Incoming frame rate from network: 62.18 FPS
Decoding frame rate: 62.18 FPS
Rendering frame rate: 62.18 FPS
Frames dropped by your network connection: 0.00%
Average network latency: 1 ms (variance: 0 ms)
Average decoding time: 4.27 ms
Average rendering time (including monitor V-sync latency): 0.06 ms
Total dropped frames: 0
\end{lstlisting}

This data shows that Moonlight is able to render all frames sent by the host without dropping any frames due to the network or decoding delays.
It is also able to keep the stream running at over 60 FPS throughout the entire session, which far exceeds the requirements of a pleasing to use system.
Taking into account the average frame decoding time of $4.27ms$ and the average rendering time of $0.06ms$, the program still has plenty of time in-between frames in case the streaming conditions are less favorable:
$(4.27ms+0.06ms) = 4.33ms < 16.\overline{66}ms = (\frac{1000ms}{60fps})$.
It is also worth noting that while testing the streaming quality, there were very few visual artifacts while monitoring the screen.
There were very few times where the screen appeared to be run through a compression filter, and it for the most part kept the visual fidelity of the host machine.

\subsection{Chrome Remote Desktop}\label{subsec:QualityCRD}

Chrome Remote Desktop also provides some quantitative data during streaming, which gives an insight into it's performance usage and speed. Figure \ref{fig:crdStats} shows screenshots of the stream during the most demanding test session.

\begin{figure}[h]
  % \centering
  % \begin{subfigure}{.25\textwidth}
  %   \centering
  %   \includegraphics[width=1\linewidth]{Figures/crd/bandwidth}
  %   \caption{Bandwidth usage in bits per second}
  %   \label{fig:crdBandwidth}
  % \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{Figures/crd/delay}
    \captionsetup{width=.85\linewidth}
    \caption{delay of image rendering in milliseconds}
    \label{fig:crdDelay}
  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{Figures/crd/fps}
    \captionsetup{width=.85\linewidth}
    \caption{Frames per second encoded by host and decoded by client}
    \label{fig:crdFPS}
  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{Figures/crd/quality}
    \captionsetup{width=.85\linewidth}
    \caption{VP8 compression quality of current frames. Lower means CRD is struggling to keep up}
    \label{fig:crdQuality}
  \end{subfigure}
  \captionsetup{width=.8\linewidth}
  \caption[CRD Data]{Statistics from CRD during the most demanding test session. Data recorded over 60 seconds.}
  \label{fig:crdStats}
\end{figure}

Unfortunately, as there is no official documentation for this data, analysis on these figures is limited but insights on the stream quality can still be found.
As seen in Figure \ref{fig:crdDelay}, the delay of image rendering climbs as the CPU is put under stress, with frames sometimes taking entire seconds to be rendered on the client machine.
It is unclear on how much of the delay is due to the host machine versus the client machine, as the delay obviously rises as the host's CPU is put under stress, but it is clear that as the host machine is under heavy load the frames can take entire seconds to be rendered on the client machine.
Figure \ref{fig:crdQuality} shows the quality setting of the VP8 compression algorithm being adjusted over time to keep up with demand.
This is a decent indication of the quality of the video stream over time, as the quality drops in an attempt to keep the stream updating the client with as much frequency as possible instead of sending full frames.
With the knowledge that CRD leverages the VP8 compression algorithm \cite{miniorange_chromoting}, it is possible to infer that as the quality goes down, the frames being sent contain less information about the current frame and instead more temporal data to construct the new frame from the previous frame.
This is also seen with visual smudging on the client's screen as the stream slows down.

\subsection{Remote Desktop Protocol}\label{subsec:QualityRDP}

Microsoft's Remote Desktop Protocol does not provide quantitative data to analyze it's quality, but speaking from a user experience perspective, and considering the data provided in the previous section, RDP performed very well under each of the testing conditions.
By dropping zero frames and visually keeping up with the host machine, it is clear that RDP works extremely well over a local network.
Similar to Moonlight, there was very little loss of visual fidelity in the client's screen compared to the host's screen.
However, noticing that the bandwidth usage can climb upwards of 10 Mb/s, such streaming may cause issues while attempting to access the host machine while outside of the local network \cite{rdp_bandwidth}.
This will be further discussed in Section \ref{sec:ConclusionLimitations}.

\subsection{Virtual Network Computing}\label{subsec:QualityVNC}

VNC provides little in the form of quality statistics, but given it's performance and responsiveness discussed in the previous section, it is clear that the quality of the stream suffers heavily whenever the host is under load.
As discussed in Section \ref{subsec:VirtualNetworkComputing}, VNC was never built to be a highly performant protocol.
The testing done for this paper really stretches the limits of the protocol and pushes the use case of VNC to the extreme, but it is still worth looking into a common and widely used tool.

\section{Real World Testing}\label{sec:RealWorldTesting}

\begin{figure}[!b]
  \centering
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/realworld/kicad}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.5\linewidth]{Figures/realworld/kicadstats}
  \end{subfigure}
  \caption[Streaming Electronics Design software]{Working on the PCB for this thesis using KiCad.}
  \label{fig:RealWorldKicad}
\end{figure}

While the testing conducted in the previous two sections provide an overview of how each application performs under controlled conditions, testing in the real world provides better insight into how a user may experience the final product.
Unfortunately, there are many limitations in how testing can be conducted in the real world.
First and foremost, due to the closed nature of of the existing solutions, collecting data while using them in real-world scenarios is impossible while leveraging the tools from the previous sections.
Because of the reliance on using exact and repeatable testing conditions, a dynamic test of each application is much more difficult to conduct without direct access to the performance of the program, all data that could be collected otherwise would amount to quantitative data subject to interpretation.
However, because of the logging tools that were developed in Section \ref{sec:DevelopingTestingAndMeasurementTools}, it is possible to collect data from the forked version of moonlight that shows the performance of the application over an entire streaming session.
While it is impossible to predict and exhaustively test what the user may try to do with the final product, it is possible to test common use cases and evaluate whether the streaming experience is satisfactory.

The first use case that was tested was streaming KiCad to the client machine to inspect the PCB layout of the custom board (Figure \ref{fig:RealWorldKicad}).
This was to test the ability to work with highly accurate CAD programs to ensure that the small client device could be used for technical work.
The first thing that was noticed was that since the chosen screen for the client device was small, everything had to be zoomed in much further than normal to be comfortable.
This is where the secondary HDMI output could be used to plug in a larger, more conventional monitor, but in the event the user did not have access to such a hardware, the small screen needed to be sufficient.
Once zoomed in, KiCad was readable and usable to without any further compromise.
The client's inputs were responsive and accurate enough for precise movements to be made without error and without the worry of needing to wait for the screen to catch up to render the changes.
Once the five minute testing session was complete, the following statistics were recorded:

\begin{lstlisting}[style=plaintext,title=Statistics recorded while streaming KiCad (Figure \ref{fig:RealWorldKicad})]
Global Video Stats
----------------------------------------------------------
Incoming frame rate from network: 61.23 FPS
Decoding frame rate: 61.23 FPS
Rendering frame rate: 61.23 FPS
Frames dropped by your network connection: 0.00%
Average network latency: 1 ms (variance: 0 ms)
Average decoding time: 4.24 ms
Average rendering time (including monitor V-sync latency): 0.06 ms
Total dropped frames: 0
\end{lstlisting}

These results are very similar to the results recorded in the previous sections, where the frame rate is able to be kept above 60 FPS throughout the test, and zero frames were dropped.

\begin{figure}[t]
  \centering
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/realworld/ai}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.5\linewidth]{Figures/realworld/aistats}
  \end{subfigure}
  \caption[Streaming the training of an AI]{Programing while training an AI model.}
  \label{fig:RealWorldAI}
\end{figure}

The next use case that was tested was programing while training an AI model (Figure \ref{fig:RealWorldAI}).
While programming on it's own is not a very intensive process, and processes such as compiling can be done while the user isn't actively interacting with the computer, training an AI model can be a very time consuming process that may need to continue in the background while the user continues to do other work.
As this scenario is extremely similar to the CPU stress test from Section \ref{sec:ResponsivityAndLatency}, the same results are expected.

\begin{lstlisting}[style=plaintext,title=Statistics recorded while training an AI (Figure \ref{fig:RealWorldAI})]
Global Video Stats
----------------------------------------------------------
Incoming frame rate from network: 62.05 FPS
Decoding frame rate: 62.05 FPS
Rendering frame rate: 62.05 FPS
Frames dropped by your network connection: 0.00%
Average network latency: 1 ms (variance: 0 ms)
Average decoding time: 4.26 ms
Average rendering time (including monitor V-sync latency): 0.05 ms
Total dropped frames: 0
\end{lstlisting}

As with before, the stream remains performant and usable throughout the test.
There was no point during the test that the stream started to struggle as the AI training process kept the CPU at 100\% usage.

\begin{figure}[t]
  \centering
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/realworld/blender}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.5\linewidth]{Figures/realworld/blenderstats}
  \end{subfigure}
  \caption[Streaming a 3D modeling, animation, and rendering program]{Working on a blender animation while rendering the scene.}
  \label{fig:RealWorldBlender}
\end{figure}

The next use case that was tested was streaming the 3D modeling, animation, and rendering program Blender (Figure \ref{fig:RealWorldBlender}).
To test this, a demo scene provided for free by the Blender Foundation was loaded and edited in one window while being rendered from the camera's point of view in real time in another window \cite{BlenderDemoScene}.
This put inconsistent strain on the computer as it constantly had to rerender the scene as changes were made.
Whenever the scene was rendered to a reasonable resolution, Blender would automatically stop rendering to keep resource usage low, resulting in the CPU and GPU usage to bounce back and forth between high and low usage.
This proved to be the most strenuous test of the application:

\begin{lstlisting}[style=plaintext,title=Statistics recorded while streaming Blender (Figure \ref{fig:RealWorldBlender})]
  Global Video Stats
----------------------------------------------------------
Incoming frame rate from network: 57.30 FPS
Decoding frame rate: 57.30 FPS
Rendering frame rate: 57.30 FPS
Frames dropped by your network connection: 0.06%
Average network latency: 1 ms (variance: 0 ms)
Average decoding time: 4.70 ms
Average rendering time (including monitor V-sync latency): 0.05 ms
Total dropped frames: 16
\end{lstlisting}

This was the only test in which not all frames rendered successfully, though the majority 99.4\% of frames rendered as expected.
In fact, due to the stream running at an average 57 FPS, the 0.06\% loss of frames was imperceivable during the test and only became known once the statistics were produced.
During the test, the stream remained performant and stable.
Though applications such as this are more difficult to use on a smaller screen, it was by no means unusable; and because of the minimal delay from the stream, there was no point during the test where it felt like the output of the tester's inputs were not visible as quick as they normally would.

\begin{figure}[t]
  \centering
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/realworld/gamedev}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[width=.5\linewidth]{Figures/realworld/gamedevstats}
  \end{subfigure}
  \caption[Streaming the development of a video game level]{Developing a video game level while inspecting it in game.}
  \label{fig:RealWorldGameDev}
\end{figure}

The last use case that was tested was streaming the development of a video game level while inspecting in the game (Figure \ref{fig:RealWorldGameDev}).
This was intended to be an general all-round stress test that would put the host under multiple kinds of stress at the same time.
Since the host computer had to run not only the video game's development tools but also the game itself, the stream had to keep up with the precise nature of geometric level design as well as interacting with the game.
During the test, the client machine always felt as though it was not compromising the experience of working on the host machine save for the small display; inputs in the development tools stayed precise and accurate, and interacting with the game was as smooth as playing on the host machine.
Once the test concluded, the following statistics were recorded:

\begin{lstlisting}[style=plaintext,title=Statistics recorded while streaming the development of a video game level (Figure \ref{fig:RealWorldGameDev})]
Global Video Stats
----------------------------------------------------------
Incoming frame rate from network: 61.93 FPS
Decoding frame rate: 61.93 FPS
Rendering frame rate: 61.93 FPS
Frames dropped by your network connection: 0.00%
Average network latency: 1 ms (variance: 0 ms)
Average decoding time: 4.27 ms
Average rendering time (including monitor V-sync latency): 0.06 ms
Total dropped frames: 0
\end{lstlisting}

As with the most of the previous tests, the stream kept a constant frame rate above 60 FPS and never dropped any frames.


\section{Summary}\label{sec:EvaluationSummary}

By combining the information from the previous three sections, a wholistic picture of the performance of each application can be drawn. Firstly, Virtual Network Computing works as a bare-bones method of connecting to a host machine from a client device in the simplest way possible, but struggles at keeping up with even mildly demanding tasks.
As seen in Figure \ref{fig:ColorDelayLoss}, VNC failed to provide a smooth experience for the user even in ideal conditions by simply not rendering updates to the screen in time for the next frame to be shown.
This, combined with a slow response time and delayed inputs, causes a poor experience for the user when attempting to use demanding applications or anything that requires speed, making it unsuitable for the purposes of this paper.

Secondly, Chrome Remote Desktop performs well under ideal conditions and even succeeds at at maintaining steady performance under some GPU load, but it struggles to keep up whenever the host's CPU is under stress.
Due to CRD's reliance on web technologies, it too suffers from the same limitations as seen in other web streaming applications such as visual smearing and dependence on the CPU.
In fact, these issues are something technology companies like Google have been trying so solve, with the release of their new VP9 codec used for YouTube starting in 2014, but such technology has not yet been introduced to CRD \cite{google_io_vp9}.
Unfortunately, due to the dependence on the CPU, and difficulty in handling conditions where the host computer is running strenuous tasks, Chrome Remote Desktop is not a suitable software solution for the purposes of this paper.

Thirdly, Microsoft's Remote Desktop Protocol proves to be a very performant protocol that can handle nearly every situation without a problem.
It is slightly CPU limited, as seen in Figure \ref{fig:InputDelay}, but even then it is able to keep input latency under a very reasonable 40ms, and it still manages to render the host's screen without dropping any frames.
It even proves to be the most stable and consistent solution in Figure \ref{fig:ColorDelay}, producing the smallest variance in the render delay of the client's screen.
If it weren't for the limitations brought up in Section \ref{subsec:RemoteDesktopProtocol}, RDP would be a great solution for streaming demanding applications to a remote client.
However, because of these drawbacks, RDP is not suitable for every application.

Finally, Moonlight proves to be a high-performant solution that isn't limited by the host's CPU or GPU being under stress.
Each test resulted in highly similar results, and while it does on average have slightly higher response times compared to RDP (Figures \ref{fig:InputDelay} and \ref{fig:ColorDelay}), it only falls behind by 5-10ms -- quicker than the blink of an eye.
By not dropping any frames, and numerically proving it's ability to keep the stream above 60 frames per second, Moonlight proves to be a great solution for streaming demanding applications to a remote client without compromising its usability.